{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea8bde",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8cf14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# init embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc411b",
   "metadata": {},
   "source": [
    "### global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824264e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_process(discription_str, current_iteration, num_of_iterations):\n",
    "    clear_output(wait=True)\n",
    "    percent = int((current_iteration/(num_of_iterations)) * 100)\n",
    "    print(discription_str+\" \"+str(percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286ca5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- configurations -------------\n",
    "MIN_CLUSTERS = 50\n",
    "MAX_CLUSTERS = 200\n",
    "\n",
    "# the name of the column that include the captions\n",
    "CAPTIONS_COLUMN_NAME = \"Caption\"\n",
    "# the name of the column that includes (will include) the posts captions embeddings\n",
    "EMBEDDINGS_COLUMN_NAME = \"caption_embedding\"\n",
    "\n",
    "\n",
    "# ------------- files url-------------\n",
    "script_outputs_folder_url = './output-data'\n",
    "script_inputs_folder_url = './input-data'\n",
    "\n",
    "posts_data_file_url = f\"{script_inputs_folder_url}/nfl_posts_by_hashtag.parquet\"\n",
    "embedded_posts_file_url = f\"{script_inputs_folder_url}/embedded_nfl_posts_by_hashtag.parquet\"\n",
    "profiles_data_file_url = f\"{script_inputs_folder_url}/nfl_profiles.parquet\"\n",
    "\n",
    "# ------------- flags -------------\n",
    "embed_posts_captions = False\n",
    "# must turn these 2 flags togther (otherswise data wont be synchronized)\n",
    "calculate_optimal_clusters_number = True\n",
    "calculate_ith_profiles_clusters_cover = True\n",
    "\n",
    "# ------------- global variables -------------\n",
    "SEPERATOR = \"=========================================================================================================\"\n",
    "OPTIMAL_CLUSTERS_NUMBER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6519b",
   "metadata": {},
   "source": [
    "## embedding\n",
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416106a",
   "metadata": {},
   "source": [
    "### embedding posts captions and storing results in a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf96acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding posts captions... 0%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m clean_caption_list\u001b[38;5;241m.\u001b[39mappend(clean_caption)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# embedding post \"clean\" caption\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m clean_caption_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclean_caption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m clean_caption_embedding_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(clean_caption_embedding))\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# printing embedding proccess percentage\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:357\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    354\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 357\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_value \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    360\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:98\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[0;32m     96\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 98\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     99\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    101\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]})\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1012\u001b[0m )\n\u001b[1;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    598\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m         output_attentions,\n\u001b[0;32m    605\u001b[0m     )\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    536\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    537\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 539\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    544\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    551\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 552\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:464\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 464\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    466\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if embed_posts_captions:\n",
    "    # data structures\n",
    "    # list to save \"clean\" captions\n",
    "    clean_caption_list = list()\n",
    "    # list to save \"clean captions\" embeddings\n",
    "    clean_caption_embedding_list = list()\n",
    "    \n",
    "    # reading NFL_posts from a parquet file into dataframe\n",
    "    posts_df = pd.read_parquet(posts_data_file_url)\n",
    "    \n",
    "    # embedding posts captions using SentenceTransformer model\n",
    "    for index, row in posts_df.iterrows():\n",
    "        # creating \"clean\" caption\n",
    "        # removing all non-asci chars ()\n",
    "        clean_caption = ''.join([char for char in row[CAPTIONS_COLUMN_NAME] if ord(char) < 128])\n",
    "        # removing all extra spaces and new-lines\n",
    "        clean_caption = ' '.join([word for word in clean_caption.split()])\n",
    "        clean_caption_list.append(clean_caption)\n",
    "        \n",
    "        # embedding post \"clean\" caption\n",
    "        clean_caption_embedding = model.encode(clean_caption)\n",
    "        clean_caption_embedding_list.append(list(clean_caption_embedding))\n",
    "        \n",
    "        # printing embedding proccess percentage\n",
    "        print_process(\"Embedding posts captions...\",index, (len(posts_df) - 1))\n",
    "        \n",
    "    # adding new columns (clean caption + embeddings) into dataframe\n",
    "    posts_df[\"Clean_Caption\"] = clean_caption_list\n",
    "    posts_df[EMBEDDINGS_COLUMN_NAME] = clean_caption_embedding_list\n",
    "    # storing dataframe into a parquet file\n",
    "    posts_df.to_parquet(embedded_posts_file_url)\n",
    "    print(\"Done saving posts with embedding to parquet file successfully\")\n",
    "else:\n",
    "    print(\"Embedding posts captions proccess is disabled!.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897577",
   "metadata": {},
   "source": [
    "#### reading posts details (data + embedded captions) and snowballed nfl teams profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9721592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading profiles data from input parquet file\n",
    "profiles_df = pd.read_parquet(profiles_data_file_url)\n",
    "profiles_df.dropna(inplace=True)\n",
    "\n",
    "# reading posts from a input parquet file\n",
    "posts_df = pd.read_parquet(embedded_posts_file_url)\n",
    "posts_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee37fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_df.info()\n",
    "display(profiles_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb019751",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_df.info()\n",
    "display(posts_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664e8d9",
   "metadata": {},
   "source": [
    "# profiles quality\n",
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f03b2",
   "metadata": {},
   "source": [
    "#### utility functions (will be used in the next cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2248a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clusters_covered_percent_by_profiles(input_profiles, input_clusters_number, input_cluster_labels):\n",
    "    \"\"\"\n",
    "    input: - profiles: a data structure that has nfl profiles\n",
    "           - input_clusters_number: the number of clusters that we have\n",
    "           - input_cluster_labels: posts clusters label array (each post to which cluster was labeled)\n",
    "    \n",
    "    the function returns the percentage of clusters covered by the profiles.\n",
    "    \"\"\"\n",
    "    # counter to count clusters that was covered by a profile\n",
    "    count = 0\n",
    "    found = False;\n",
    "\n",
    "    # scanning all clusters\n",
    "    for cluster in range(input_clusters_number):\n",
    "        vectors_indexes_in_cluster = np.where(input_cluster_labels == cluster)[0]\n",
    "\n",
    "        # scanning all vectors in the cluster\n",
    "        for idx in vectors_indexes_in_cluster:  \n",
    "            # get the details of the post corresponding to the vector\n",
    "            post_details_row = posts_df[posts_df[\"ID\"] == posts_id_arr[idx]]\n",
    "            # get the id of the post owner\n",
    "            owner_id = post_details_row[\"Owner_ID\"].iloc[0]\n",
    "            # Verify whether the post owner exists in the list of profiles\n",
    "            if owner_id in input_profiles:\n",
    "                count = count + 1\n",
    "                break\n",
    "                \n",
    "    return ((count/input_clusters_number) * 100)\n",
    "\n",
    "\n",
    "def find_optimal_num_clusters(vectors, min_clusters=1, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Find the optimal number of clusters using the elbow method.\n",
    "\n",
    "    Args:\n",
    "    - vectors (list of arrays): List of 340-dimensional vectors.\n",
    "    - max_clusters (int): Maximum number of clusters to consider.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_num_clusters (int): Optimal number of clusters.\n",
    "    \"\"\"\n",
    "    # Calculate within-cluster sum of squares (WCSS) for different number of clusters and clusters covered by profiles\n",
    "    wcss = []\n",
    "    clusters_covered = []\n",
    "    \n",
    "    for num_clusters in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        kmeans.fit(vectors)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        # calculating number of clusters covered by profiles\n",
    "        clusters_covered.append(calculate_clusters_covered_percent_by_profiles(profiles_id_arr, num_clusters, kmeans.labels_))\n",
    "        \n",
    "        # printing proccess percentage\n",
    "        print_process(\"Calculating optimal number of clusters...\",num_clusters, max_clusters)\n",
    "    \n",
    "    # saving results into dataframe\n",
    "    clusters_range = range(min_clusters, max_clusters + 1)\n",
    "    optimal_clustering_details_df = pd.DataFrame({\"clusters_number\":clusters_range,\n",
    "                                               \"wcss\":wcss,\n",
    "                                               \"clusters_percentage_covered\": clusters_covered}).set_index('clusters_number')\n",
    "    # saving dataframe into a csv file\n",
    "    optimal_clustering_details_df.to_csv(f\"{script_outputs_folder_url}/optimal-num-of-clusters-calculation-results.csv\")\n",
    "    \n",
    "    # Find the elbow point\n",
    "    diff = np.diff(wcss, 2)\n",
    "    optimal_num_clusters = np.argmin(diff) + min_clusters + 1\n",
    "\n",
    "    return optimal_num_clusters\n",
    "\n",
    "\n",
    "def print_optimal_num_of_clusters_calculation_graphs(input_range, input_wcss, input_clusters_covered):\n",
    "    \"\"\"\n",
    "    input: -input_range: the range from 1 to MAX number of clusters\n",
    "           -input_wcss: wcss values for each number of clusters\n",
    "           -input_clusters_covered: covered clusters by profiles percent for each number of number of clusters\n",
    "           \n",
    "    the functions plot elbow curve and clusters covered percantage graphs\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(input_range, input_wcss)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Within-cluster sum of squares (WCSS)')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(input_range, input_clusters_covered)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Covered clusters percent')\n",
    "    plt.title('Clusters Covered By Profiles')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df34dee",
   "metadata": {},
   "source": [
    "## building KMeans model for the clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbf8be",
   "metadata": {},
   "source": [
    "#### reading relevant columns to build the model from dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cd5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading embeddings column from posts dataframe into a nparray \n",
    "posts_embedding_vectors_arr = np.array(posts_df[EMBEDDINGS_COLUMN_NAME].apply(np.array).tolist())\n",
    "\n",
    "# reading posts id column from posts dataframe into a nparray\n",
    "posts_id_arr = np.array(posts_df['ID'].apply(np.array).tolist())\n",
    "\n",
    "# reading profiles id column from profilesdf into a nparray\n",
    "profiles_id_arr = np.array(profiles_df['ID'].apply(np.array).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65847957",
   "metadata": {},
   "source": [
    "#### calculating optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104bfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_optimal_clusters_number:\n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    OPTIMAL_CLUSTERS_NUMBER = find_optimal_num_clusters(posts_embedding_vectors_arr, MIN_CLUSTERS, MAX_CLUSTERS)\n",
    "    optimal_clusters_num_df = pd.DataFrame({\"-\":\"Optimal number of clusters\", 'Value': [str(OPTIMAL_CLUSTERS_NUMBER)]}).set_index('-')\n",
    "    optimal_clusters_num_df.to_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\")\n",
    "else:\n",
    "    print(\"Calculating optional number of clusters proccess is disabled!.\")\n",
    "    # reading optiomal number of clusters from last calculation\n",
    "    OPTIMAL_CLUSTERS_NUMBER = pd.read_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\")[\"Value\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing optimal number of clusters using dataframe\n",
    "optimal_clusters_num_df = pd.read_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\").set_index('-')\n",
    "display(optimal_clusters_num_df)\n",
    "\n",
    "# reading optimal clusters number calculating results df\n",
    "optimal_clustering_details_df = pd.read_csv(f\"{script_outputs_folder_url}/optimal-num-of-clusters-calculation-results.csv\")\n",
    "# Plot the elbow curve and clusters covered percentage\n",
    "print_optimal_num_of_clusters_calculation_graphs(optimal_clustering_details_df[\"clusters_number\"],\n",
    "                                                 optimal_clustering_details_df[\"wcss\"],\n",
    "                                                 optimal_clustering_details_df[\"clusters_percentage_covered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab2aba",
   "metadata": {},
   "source": [
    "#### building kmeans model with optimal number of clusters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KMeans model with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_CLUSTERS_NUMBER, random_state=42)\n",
    "    \n",
    "# Fit KMeans model to the vectors\n",
    "kmeans.fit(posts_embedding_vectors_arr)\n",
    "\n",
    "# Get cluster labels assigned to each vector\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "# extracting posts indexes per cluster\n",
    "# --> posts_per_cluster_lst[i] will include posts indexes that belong to cluster i\n",
    "\n",
    "posts_indexes_per_cluster_lst = list()\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    posts_indexes_per_cluster_lst.append([index for index, value in enumerate(cluster_labels) if value == cluster])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74091a60",
   "metadata": {},
   "source": [
    "#### covered clusters (by snowballed profiles) percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating snowballed profiles quality percent\n",
    "percent = calculate_clusters_covered_percent_by_profiles(profiles_id_arr, OPTIMAL_CLUSTERS_NUMBER, cluster_labels)\n",
    "\n",
    "#printing result using dataframe\n",
    "display(pd.DataFrame({\"-\":\"Clusters Covered\", 'Percentage': [\"{:.2f}\".format(percent)+\"%\"]}).set_index('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a32ad",
   "metadata": {},
   "source": [
    "#### i'th profiles quality grapth (covered clusters percent for the first i'th profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_ith_profiles_clusters_cover:\n",
    "    # reading posts Owners_Id column from posts dataframe into a nparray\n",
    "    posts_owners_id_arr = np.array(posts_df['Owner_ID'].apply(np.array).tolist())\n",
    "\n",
    "    # list to save percentage results\n",
    "    percentage_lst = list()\n",
    "\n",
    "    # calculating profiles dataframe length\n",
    "    profiles_count = len(profiles_id_arr)\n",
    "\n",
    "    for curr_num_of_profiles in range(1, profiles_count + 1):\n",
    "        # initializing a set that include all the first i profiles id\n",
    "        curr_profiles_id_set = set(profiles_id_arr[:curr_num_of_profiles])\n",
    "\n",
    "        #scanning clusters to check how many were covered by the first ith profiles\n",
    "        clusters_covered_count = 0\n",
    "        for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "            # extracting posts owners id in the current cluster\n",
    "            posts_owners_id_in_cluster_set = set([posts_owners_id_arr[idx] for idx in posts_indexes_per_cluster_lst[cluster]])\n",
    "            # checking if the cluster was covered by the current profiles set\n",
    "            intersaction_set = curr_profiles_id_set.intersection(posts_owners_id_in_cluster_set)\n",
    "            if len(intersaction_set) > 0:\n",
    "                clusters_covered_count = clusters_covered_count + 1\n",
    "\n",
    "        percentage_lst.append((clusters_covered_count / OPTIMAL_CLUSTERS_NUMBER) * 100)\n",
    "        #printing process percentage\n",
    "        print_process(\"Calculating profiles quality...\", curr_num_of_profiles, profiles_count)\n",
    "\n",
    "\n",
    "    # creating a list that include the number 1 to len(profiles_df)\n",
    "    amount_of_profiles_lst = list(range(1, len(profiles_id_arr) + 1))\n",
    "\n",
    "    # creating dataframe with results\n",
    "    covered_clusters_results_df = pd.DataFrame({'profiles_count': amount_of_profiles_lst,\n",
    "                                   'percentage': percentage_lst})\n",
    "    #saving results into a csv file\n",
    "    covered_clusters_results_df.to_csv(f\"{script_outputs_folder_url}/covered-clusters-i-profiles.csv\")\n",
    "\n",
    "\n",
    "# plot first i'th profiles cover results graph\n",
    "covered_clusters_results_df = pd.read_csv(f\"{script_outputs_folder_url}/covered-clusters-i-profiles.csv\")\n",
    "plt.plot(covered_clusters_results_df[\"profiles_count\"], covered_clusters_results_df[\"percentage\"])\n",
    "plt.xlabel(\"amount of profiles\")  # Set x-axis label\n",
    "plt.ylabel(\"Clusters Covered percentage %\")  # Set y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e573c6",
   "metadata": {},
   "source": [
    "#### creating clusters captions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b105756",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_captions_list = list()\n",
    "all_captions_list = posts_df[CAPTIONS_COLUMN_NAME]\n",
    "columns_names = list()\n",
    "\n",
    "# scanning clusters to save each post caption in specific cluster\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # extracting all captions that belongs to the current cluster\n",
    "    cluster_captions_list = [all_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "    columns_names.append(f\"cluster_{cluster+1}\")\n",
    "    \n",
    "\n",
    "# Transpose the list of lists to switch rows with columns\n",
    "transposed_lists = list(map(list, zip(*cluster_captions_list)))\n",
    "# Create a DataFrame from the transposed list with optional column names\n",
    "clusters_captions_df = pd.DataFrame(transposed_lists, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9404f5d",
   "metadata": {},
   "source": [
    "#### saving dataframe into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d3f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_captions_df.info()\n",
    "display(clusters_captions_df.head(5))\n",
    "clusters_captions_df.to_parquet(f\"{script_outputs_folder_url}/clusters-captions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f59076",
   "metadata": {},
   "source": [
    "## Naming clusters\n",
    "#### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1112bb6d",
   "metadata": {},
   "source": [
    "### method 1: most common triple words in cluster (using black list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30aa92",
   "metadata": {},
   "source": [
    "#### Preparing a blacklist of words that we wish to exclude from our cluster name tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f74b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\"a\", \"an\", \"the\"]\n",
    "conjunctions = [\"and\", \"but\", \"or\", \"nor\", \"for\", \"yet\", \"so\", \"after\", \"although\", \"as\", \"because\", \"before\", \"if\", \"since\", \"though\", \"unless\", \"until\", \"when\", \"where\", \"while\"]\n",
    "prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"around\",\n",
    "                \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\",\n",
    "                \"by\", \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"excepting\",\n",
    "                \"excluding\", \"following\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\",\n",
    "                \"on\", \"onto\", \"out\", \"outside\", \"over\", \"past\", \"regarding\", \"round\", \"since\", \"through\",\n",
    "                \"throughout\", \"till\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"until\", \"unto\",\n",
    "                \"up\", \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\"]\n",
    "pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"you\", \"him\", \"her\", \"us\", \"them\",\n",
    "            \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\",\n",
    "            \"who\", \"whom\", \"whose\", \"which\", \"what\", \"that\", \"whichever\", \"whatever\", \"whoever\", \"whomever\",\n",
    "            \"this\", \"these\", \"those\", \"someone\", \"somebody\", \"something\", \"anyone\", \"anybody\", \"anything\",\n",
    "            \"everyone\", \"everybody\", \"everything\", \"no one\", \"nobody\", \"nothing\", \"each\", \"either\", \"neither\",\n",
    "            \"one\", \"other\", \"another\", \"such\", \"much\", \"few\", \"both\", \"all\", \"any\", \"some\", \"several\", \"none\",\n",
    "            \"every\", \"many\", \"more\", \"most\", \"enough\", \"little\", \"fewer\", \"fewest\", \"least\", \"I\", \"you\", \"he\",\n",
    "            \"she\", \"it\", \"we\", \"they\"]\n",
    "possessive_determiners = [\"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"your\", \"their\"]\n",
    "randoms = [\"is\", \"are\", \"yes\", \"no\", \"http\", \"https\", \"has\", \"had\", \"will\", \"would\", \"was\", \"were\", \"not\",\n",
    "           \"be\", \"just\", \"been\", \"do\", \"does\", \"has\", \"have\"]\n",
    "\n",
    "common_words_set = set(articles + conjunctions + prepositions + pronouns + possessive_determiners + randoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee111b0",
   "metadata": {},
   "source": [
    "#### coverting each caption into unique words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74131983",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a list to store aggresive cleaned posts text - removing symbols and numbers\n",
    "clean_captions_list = list()\n",
    "# a list to save each caption words \n",
    "clean_captions_unique_words_list = list()\n",
    "\n",
    "# scanning posts dataframe to aggresive clean posts captions\n",
    "for idx, row in posts_df.iterrows():\n",
    "    # aggresive cleaning the texts, removing all numbers and symbols\n",
    "    # reading caption\n",
    "    caption = row[CAPTIONS_COLUMN_NAME]\n",
    "    # removing all non-alphabeta chars\n",
    "    clean_caption = ''.join([char if char.isalpha() else ' ' for char in caption])\n",
    "    clean_caption = ' '.join([word for word in clean_caption.split() if (word.isalpha() and len(word) >=3)])\n",
    "    clean_captions_list.append(clean_caption)\n",
    "\n",
    "\n",
    "for i in range(len(clean_captions_list)):\n",
    "    # coverting clean post texts into a unique list of words\n",
    "    clean_captions_unique_words_list.append(list(set([word for word in clean_captions_list[i].lower().split() if (word not in common_words_set)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cb335",
   "metadata": {},
   "source": [
    "#### scanning clusters to find most common 3 words tupple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51293dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save clusters name\n",
    "clusters_name_lst = list()\n",
    "\n",
    "# scanning clusters\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # init a dictionary to count words tuple appearances\n",
    "    words_tupple_count_dict = dict()\n",
    "    # extracting cluster caption sentences (each caption is a list of words)\n",
    "    cluster_captions_words = [clean_captions_unique_words_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "\n",
    "    # scanning all captions in cluster to count words tuples appearances\n",
    "    for caption_words in cluster_captions_words:\n",
    "        # checking if caption has enough words\n",
    "        if len(caption_words) >= 3:\n",
    "            for idx1 in range(len(caption_words)-2):\n",
    "                for idx2 in range(idx1+1, len(caption_words)-1):\n",
    "                    for idx3 in range(idx2+1, len(caption_words)):\n",
    "                        words_tuple = (caption_words[idx1], caption_words[idx2], caption_words[idx3])\n",
    "                        current_count = words_tupple_count_dict.get(words_tuple, 0)\n",
    "                        words_tupple_count_dict[words_tuple] = current_count + 1\n",
    "    \n",
    "    # adding cluster name tuple to list\n",
    "    clusters_name_lst.append(max(words_tupple_count_dict, key=words_tupple_count_dict.get))\n",
    "    \n",
    "    # printing proccess percentage\n",
    "    print_process(\"Producing names to clusters...\", cluster, OPTIMAL_CLUSTERS_NUMBER - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d4cc8",
   "metadata": {},
   "source": [
    "#### printing clusters names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_names_df = pd.DataFrame({'cluster number':[i for i in range(OPTIMAL_CLUSTERS_NUMBER)],\n",
    "                                  'cluster name': clusters_name_lst }).set_index(\"cluster number\")\n",
    "\n",
    "clusters_names_file_url = data_set_folder_url +\"cluster-names.csv\"\n",
    "clusters_names_df.to_csv(clusters_names_file_url)\n",
    "display(clusters_names_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6a9ab",
   "metadata": {},
   "source": [
    "#### Creating cluster's captions dataframe, saving it to file and printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2de39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e522dc",
   "metadata": {},
   "source": [
    "### method 2: td-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141297e",
   "metadata": {},
   "source": [
    "#### creating clusters decuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_documents_list = list()\n",
    "all_captions_list = posts_df[CAPTIONS_COLUMN_NAME]\n",
    "\n",
    "# scanning clusters to build document for each cluster\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # extracting all captions that belongs to the current cluster\n",
    "    cluster_captions_list = [all_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "    # creating current cluster document (appending all captions togther)\n",
    "    clusters_documents_list[cluster] = ' '.join(cluster_captions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "# the return value is a matrix:\n",
    "#   - row i describe the tfidf score of each word in document i\n",
    "#   - column j describe the tfidf score for word j in each document\n",
    "# the order of the columns correspond to the words list we receive from vectorizer.get_feature_names_out() function\n",
    "tfidf_score_array = vectorizer.fit_transform(clusters_documents_list).toarray()\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf score dataframe\n",
    "# - column i include the tf-idf score for term i in all documents\n",
    "# - row i include terms score in document i\n",
    "# terms_tfidf_score_in_documents_df(i,j) = tf-idf score for term j in document i\n",
    "terms_tfidf_score_in_documents_df = pd.DataFrame(tfidf_score_array, columns = feature_names)\n",
    "\n",
    "display(terms_tfidf_score_in_documents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc19a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(feature_names):\n",
    "    print(str(i)+\". \"+word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d895f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==0, \"end of program :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b5f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45800ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code that would be needed for td-idf\n",
    "\n",
    "# dict to each cluster captions (key-cluster number, value-captions list)\n",
    "# will be used to save clusters caption in csv file and for tf-idf documents\n",
    "cluster_captions_dict = dict()\n",
    "\n",
    "# saving clusters clean captions\n",
    "cluster_captions = [clean_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "cluster_captions_dict[cluster_dict_key] = cluster_captions\n",
    "\n",
    "# soft clean code\n",
    "\n",
    "clean_caption = ''.join([char for char in row[CAPTIONS_COLUMN_NAME] if ord(char) < 128])\n",
    "# removing all extra spaces and new-lines\n",
    "clean_caption = ' '.join([word for word in clean_caption.split()])\n",
    "\n",
    "\n",
    "### need to remmber what does this code do!\n",
    "# a list to store terms-score dictionary for each cluster\n",
    "cluster_terms_score_dict_list = list()\n",
    "\n",
    "for idx in range(len(tfidf_score_array)):\n",
    "    cluster_terms_score_dict = dict(zip(feature_names, tfidf_score_array[idx]))\n",
    "    # removing common words from dict\n",
    "    cluster_terms_score_dict = {k: v for k, v in cluster_terms_score_dict.items() if k not in common_words_set}\n",
    "    # Sort the dictionary by values in descending order\n",
    "    cluster_terms_score_dict = sorted(cluster_terms_score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # append dictionary to dictionaries list\n",
    "    cluster_terms_score_dict_list.append(cluster_terms_score_dict)\n",
    "    \n",
    "cluster_terms_score_dict_list[1]\n",
    "\n",
    "#### code to create df which include clusters captions\n",
    "# creating df from dict\n",
    "cluster_captions_df = pd.concat({key: pd.Series(vals) for key, vals in cluster_captions_dict.items()}, axis=1)\n",
    "# saving into csv file\n",
    "clusters_caption_file_url = data_set_folder_url +\"clusters-captions.csv\"\n",
    "cluster_captions_df.to_csv(clusters_caption_file_url)\n",
    "# display dataframe for fast preview\n",
    "display(cluster_captions_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca22b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf248f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['data science is one of the most important fields of science',\n",
    "          'this is one of the best data science courses',\n",
    "          'data scientists analyze data' ]\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer(max_df = 0.5)\n",
    "tf_idf_array = tr_idf_model.fit_transform(corpus).toarray()\n",
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "\n",
    "display(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "for doc in  documents:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "n_docs = len(documents)         #·Number of documents in the corpus\n",
    "n_words_set = len(words_set) #·Number of unique words in the \n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_set)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muhamenv)",
   "language": "python",
   "name": "muhamenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
