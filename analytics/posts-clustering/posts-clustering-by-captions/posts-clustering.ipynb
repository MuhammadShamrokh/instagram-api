{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ea8bde",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef8cf14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\anaconda3\\envs\\muhamenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from IPython.display import clear_output\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# init embedding model\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc411b",
   "metadata": {},
   "source": [
    "### global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "824264e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_process(discription_str, current_iteration, num_of_iterations):\n",
    "    clear_output(wait=True)\n",
    "    percent = int((current_iteration/(num_of_iterations)) * 100)\n",
    "    print(discription_str+\" \"+str(percent)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "286ca5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------- configurations -------------\n",
    "MIN_CLUSTERS = 50\n",
    "MAX_CLUSTERS = 200\n",
    "\n",
    "# the name of the column that include the captions\n",
    "CAPTIONS_COLUMN_NAME = \"Caption\"\n",
    "# the name of the column that includes (will include) the posts captions embeddings\n",
    "EMBEDDINGS_COLUMN_NAME = \"caption_embedding\"\n",
    "\n",
    "\n",
    "# ------------- files url-------------\n",
    "script_outputs_folder_url = './output-data'\n",
    "script_inputs_folder_url = './input-data'\n",
    "\n",
    "posts_data_file_url = f\"{script_inputs_folder_url}/nfl_posts_by_hashtag.parquet\"\n",
    "embedded_posts_file_url = f\"{script_inputs_folder_url}/embedded_nfl_posts_by_hashtag.parquet\"\n",
    "profiles_data_file_url = f\"{script_inputs_folder_url}/nfl_profiles.parquet\"\n",
    "\n",
    "# ------------- flags -------------\n",
    "embed_posts_captions = False\n",
    "# must turn these 2 flags togther (otherswise data wont be synchronized)\n",
    "calculate_optimal_clusters_number = True\n",
    "calculate_ith_profiles_clusters_cover = True\n",
    "\n",
    "# ------------- global variables -------------\n",
    "SEPERATOR = \"=========================================================================================================\"\n",
    "OPTIMAL_CLUSTERS_NUMBER = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6519b",
   "metadata": {},
   "source": [
    "## embedding\n",
    "#### ------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c416106a",
   "metadata": {},
   "source": [
    "### embedding posts captions and storing results in a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf96acde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding posts captions proccess is disabled!.\n"
     ]
    }
   ],
   "source": [
    "if embed_posts_captions:\n",
    "    # data structures\n",
    "    # list to save \"clean\" captions\n",
    "    clean_caption_list = list()\n",
    "    # list to save \"clean captions\" embeddings\n",
    "    clean_caption_embedding_list = list()\n",
    "    \n",
    "    # reading NFL_posts from a parquet file into dataframe\n",
    "    posts_df = pd.read_parquet(posts_data_file_url)\n",
    "    \n",
    "    # embedding posts captions using SentenceTransformer model\n",
    "    for index, row in posts_df.iterrows():\n",
    "        # creating \"clean\" caption\n",
    "        # removing all non-asci chars ()\n",
    "        clean_caption = ''.join([char for char in row[CAPTIONS_COLUMN_NAME] if ord(char) < 128])\n",
    "        # removing all extra spaces and new-lines\n",
    "        clean_caption = ' '.join([word for word in clean_caption.split()])\n",
    "        clean_caption_list.append(clean_caption)\n",
    "        \n",
    "        # embedding post \"clean\" caption\n",
    "        clean_caption_embedding = model.encode(clean_caption)\n",
    "        clean_caption_embedding_list.append(list(clean_caption_embedding))\n",
    "        \n",
    "        # printing embedding proccess percentage\n",
    "        print_process(\"Embedding posts captions...\",index, (len(posts_df) - 1))\n",
    "        \n",
    "    # adding new columns (clean caption + embeddings) into dataframe\n",
    "    posts_df[\"Clean_Caption\"] = clean_caption_list\n",
    "    posts_df[EMBEDDINGS_COLUMN_NAME] = clean_caption_embedding_list\n",
    "    # storing dataframe into a parquet file\n",
    "    posts_df.to_parquet(embedded_posts_file_url)\n",
    "    print(\"Done saving posts with embedding to parquet file successfully\")\n",
    "else:\n",
    "    print(\"Embedding posts captions proccess is disabled!.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6897577",
   "metadata": {},
   "source": [
    "#### reading posts details (data + embedded captions) and snowballed nfl teams profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9721592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading profiles data from input parquet file\n",
    "profiles_df = pd.read_parquet(profiles_data_file_url)\n",
    "profiles_df.dropna(inplace=True)\n",
    "\n",
    "# reading posts from a input parquet file\n",
    "posts_df = pd.read_parquet(embedded_posts_file_url)\n",
    "posts_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee37fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11325 entries, 0 to 11326\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   ID                   11325 non-null  object \n",
      " 1   Nickname             11325 non-null  object \n",
      " 2   Post_Count           11325 non-null  float64\n",
      " 3   Follower_Count       11325 non-null  float64\n",
      " 4   Following_Count      11325 non-null  int64  \n",
      " 5   Posts_Count_In_Week  11325 non-null  int64  \n",
      " 6   Engagement           11325 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 707.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Post_Count</th>\n",
       "      <th>Follower_Count</th>\n",
       "      <th>Following_Count</th>\n",
       "      <th>Posts_Count_In_Week</th>\n",
       "      <th>Engagement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28995773</td>\n",
       "      <td>bleacherreport</td>\n",
       "      <td>53286.0</td>\n",
       "      <td>22282444.0</td>\n",
       "      <td>713</td>\n",
       "      <td>169</td>\n",
       "      <td>42912907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>205593849</td>\n",
       "      <td>nfl</td>\n",
       "      <td>59871.0</td>\n",
       "      <td>29921744.0</td>\n",
       "      <td>2204</td>\n",
       "      <td>161</td>\n",
       "      <td>33917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1254997058</td>\n",
       "      <td>houseofhighlights</td>\n",
       "      <td>31428.0</td>\n",
       "      <td>51962379.0</td>\n",
       "      <td>2185</td>\n",
       "      <td>110</td>\n",
       "      <td>28233366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>253785656</td>\n",
       "      <td>nflnetwork</td>\n",
       "      <td>16008.0</td>\n",
       "      <td>3984113.0</td>\n",
       "      <td>506</td>\n",
       "      <td>77</td>\n",
       "      <td>12237502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1254050784</td>\n",
       "      <td>espnnfl</td>\n",
       "      <td>22638.0</td>\n",
       "      <td>3948795.0</td>\n",
       "      <td>603</td>\n",
       "      <td>85</td>\n",
       "      <td>11310141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ID           Nickname  Post_Count  Follower_Count  Following_Count  \\\n",
       "0    28995773     bleacherreport     53286.0      22282444.0              713   \n",
       "1   205593849                nfl     59871.0      29921744.0             2204   \n",
       "2  1254997058  houseofhighlights     31428.0      51962379.0             2185   \n",
       "3   253785656         nflnetwork     16008.0       3984113.0              506   \n",
       "4  1254050784            espnnfl     22638.0       3948795.0              603   \n",
       "\n",
       "   Posts_Count_In_Week  Engagement  \n",
       "0                  169    42912907  \n",
       "1                  161    33917101  \n",
       "2                  110    28233366  \n",
       "3                   77    12237502  \n",
       "4                   85    11310141  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "profiles_df.info()\n",
    "display(profiles_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb019751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12518 entries, 0 to 12517\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   ID                 12518 non-null  object\n",
      " 1   Caption            12518 non-null  object\n",
      " 2   Owner_ID           12518 non-null  object\n",
      " 3   Likes_Count        12518 non-null  int64 \n",
      " 4   Comments_Count     12518 non-null  int64 \n",
      " 5   publication_Date   12518 non-null  object\n",
      " 6   Clean_Caption      12518 non-null  object\n",
      " 7   caption_embedding  12518 non-null  object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 782.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Caption</th>\n",
       "      <th>Owner_ID</th>\n",
       "      <th>Likes_Count</th>\n",
       "      <th>Comments_Count</th>\n",
       "      <th>publication_Date</th>\n",
       "      <th>Clean_Caption</th>\n",
       "      <th>caption_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3321924617986818393</td>\n",
       "      <td>The Bills are re-signing DT DaQuan Jones. 2 ye...</td>\n",
       "      <td>9174240239</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>The Bills are re-signing DT DaQuan Jones. 2 ye...</td>\n",
       "      <td>[-0.05456706, -0.028283583, -0.012744162, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3321927450987513283</td>\n",
       "      <td>🔥Customized Sharing（49/100）🔥\\n✈delivered\\n\\nBi...</td>\n",
       "      <td>51714893107</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>Customized Sharing49/100 delivered Bills jerse...</td>\n",
       "      <td>[-0.06672458, 0.113558196, -0.02331945, -0.052...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3321937043117426576</td>\n",
       "      <td>The Bills have resigned Daquan Jones on a 2 ye...</td>\n",
       "      <td>52222828237</td>\n",
       "      <td>22</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>The Bills have resigned Daquan Jones on a 2 ye...</td>\n",
       "      <td>[-0.047176495, 0.012292344, -0.010614566, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3321969411785703617</td>\n",
       "      <td>DT Daquon Jones bleibt für zwei weitere Jahre ...</td>\n",
       "      <td>38454064620</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>DT Daquon Jones bleibt fr zwei weitere Jahre u...</td>\n",
       "      <td>[-0.06138219, -0.038200036, -0.03786822, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3321989952400201853</td>\n",
       "      <td>Khalil Shakir Orange 🍊 62/249 from 2023 Panini...</td>\n",
       "      <td>8128714351</td>\n",
       "      <td>7</td>\n",
       "      <td>-1</td>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>Khalil Shakir Orange 62/249 from 2023 Panini P...</td>\n",
       "      <td>[-0.040837407, -0.014204753, -0.0047751833, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                            Caption  \\\n",
       "0  3321924617986818393  The Bills are re-signing DT DaQuan Jones. 2 ye...   \n",
       "1  3321927450987513283  🔥Customized Sharing（49/100）🔥\\n✈delivered\\n\\nBi...   \n",
       "2  3321937043117426576  The Bills have resigned Daquan Jones on a 2 ye...   \n",
       "3  3321969411785703617  DT Daquon Jones bleibt für zwei weitere Jahre ...   \n",
       "4  3321989952400201853  Khalil Shakir Orange 🍊 62/249 from 2023 Panini...   \n",
       "\n",
       "      Owner_ID  Likes_Count  Comments_Count publication_Date  \\\n",
       "0   9174240239           -1              -1       2024-03-12   \n",
       "1  51714893107           -1              -1       2024-03-12   \n",
       "2  52222828237           22              -1       2024-03-12   \n",
       "3  38454064620            4              -1       2024-03-12   \n",
       "4   8128714351            7              -1       2024-03-12   \n",
       "\n",
       "                                       Clean_Caption  \\\n",
       "0  The Bills are re-signing DT DaQuan Jones. 2 ye...   \n",
       "1  Customized Sharing49/100 delivered Bills jerse...   \n",
       "2  The Bills have resigned Daquan Jones on a 2 ye...   \n",
       "3  DT Daquon Jones bleibt fr zwei weitere Jahre u...   \n",
       "4  Khalil Shakir Orange 62/249 from 2023 Panini P...   \n",
       "\n",
       "                                   caption_embedding  \n",
       "0  [-0.05456706, -0.028283583, -0.012744162, -0.0...  \n",
       "1  [-0.06672458, 0.113558196, -0.02331945, -0.052...  \n",
       "2  [-0.047176495, 0.012292344, -0.010614566, -0.0...  \n",
       "3  [-0.06138219, -0.038200036, -0.03786822, -0.05...  \n",
       "4  [-0.040837407, -0.014204753, -0.0047751833, -0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "posts_df.info()\n",
    "display(posts_df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8664e8d9",
   "metadata": {},
   "source": [
    "# profiles quality\n",
    "#### -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f03b2",
   "metadata": {},
   "source": [
    "#### utility functions (will be used in the next cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2248a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clusters_covered_percent_by_profiles(input_profiles, input_clusters_number, input_cluster_labels):\n",
    "    \"\"\"\n",
    "    input: - profiles: a data structure that has nfl profiles\n",
    "           - input_clusters_number: the number of clusters that we have\n",
    "           - input_cluster_labels: posts clusters label array (each post to which cluster was labeled)\n",
    "    \n",
    "    the function returns the percentage of clusters covered by the profiles.\n",
    "    \"\"\"\n",
    "    # counter to count clusters that was covered by a profile\n",
    "    count = 0\n",
    "    found = False;\n",
    "\n",
    "    # scanning all clusters\n",
    "    for cluster in range(input_clusters_number):\n",
    "        vectors_indexes_in_cluster = np.where(input_cluster_labels == cluster)[0]\n",
    "\n",
    "        # scanning all vectors in the cluster\n",
    "        for idx in vectors_indexes_in_cluster:  \n",
    "            # get the details of the post corresponding to the vector\n",
    "            post_details_row = posts_df[posts_df[\"ID\"] == posts_id_arr[idx]]\n",
    "            # get the id of the post owner\n",
    "            owner_id = post_details_row[\"Owner_ID\"].iloc[0]\n",
    "            # Verify whether the post owner exists in the list of profiles\n",
    "            if owner_id in input_profiles:\n",
    "                count = count + 1\n",
    "                break\n",
    "                \n",
    "    return ((count/input_clusters_number) * 100)\n",
    "\n",
    "\n",
    "def find_optimal_num_clusters(vectors, min_clusters=1, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Find the optimal number of clusters using the elbow method.\n",
    "\n",
    "    Args:\n",
    "    - vectors (list of arrays): List of 340-dimensional vectors.\n",
    "    - max_clusters (int): Maximum number of clusters to consider.\n",
    "\n",
    "    Returns:\n",
    "    - optimal_num_clusters (int): Optimal number of clusters.\n",
    "    \"\"\"\n",
    "    # Calculate within-cluster sum of squares (WCSS) for different number of clusters and clusters covered by profiles\n",
    "    wcss = []\n",
    "    clusters_covered = []\n",
    "    \n",
    "    for num_clusters in range(min_clusters, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        kmeans.fit(vectors)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        # calculating number of clusters covered by profiles\n",
    "        clusters_covered.append(calculate_clusters_covered_percent_by_profiles(profiles_id_arr, num_clusters, kmeans.labels_))\n",
    "        \n",
    "        # printing proccess percentage\n",
    "        print_process(\"Calculating optimal number of clusters...\",(num_clusters - min_clusters + 1), (max_clusters - min_clusters))\n",
    "    \n",
    "    # saving results into dataframe\n",
    "    clusters_range = range(min_clusters, max_clusters + 1)\n",
    "    optimal_clustering_details_df = pd.DataFrame({\"clusters_number\":clusters_range,\n",
    "                                               \"wcss\":wcss,\n",
    "                                               \"clusters_percentage_covered\": clusters_covered}).set_index('clusters_number')\n",
    "    # saving dataframe into a csv file\n",
    "    optimal_clustering_details_df.to_csv(f\"{script_outputs_folder_url}/optimal-num-of-clusters-calculation-results.csv\")\n",
    "    \n",
    "    # Find the elbow point\n",
    "    diff = np.diff(wcss, 2)\n",
    "    optimal_num_clusters = np.argmin(diff) + min_clusters + 1\n",
    "\n",
    "    return optimal_num_clusters\n",
    "\n",
    "\n",
    "def print_optimal_num_of_clusters_calculation_graphs(input_range, input_wcss, input_clusters_covered):\n",
    "    \"\"\"\n",
    "    input: -input_range: the range from 1 to MAX number of clusters\n",
    "           -input_wcss: wcss values for each number of clusters\n",
    "           -input_clusters_covered: covered clusters by profiles percent for each number of number of clusters\n",
    "           \n",
    "    the functions plot elbow curve and clusters covered percantage graphs\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.plot(input_range, input_wcss)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Within-cluster sum of squares (WCSS)')\n",
    "    plt.title('Elbow Method')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(input_range, input_clusters_covered)\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Covered clusters percent')\n",
    "    plt.title('Clusters Covered By Profiles')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df34dee",
   "metadata": {},
   "source": [
    "## building KMeans model for the clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dbf8be",
   "metadata": {},
   "source": [
    "#### reading relevant columns to build the model from dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84cd5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading embeddings column from posts dataframe into a nparray \n",
    "posts_embedding_vectors_arr = np.array(posts_df[EMBEDDINGS_COLUMN_NAME].apply(np.array).tolist())\n",
    "\n",
    "# reading posts id column from posts dataframe into a nparray\n",
    "posts_id_arr = np.array(posts_df['ID'].apply(np.array).tolist())\n",
    "\n",
    "# reading profiles id column from profilesdf into a nparray\n",
    "profiles_id_arr = np.array(profiles_df['ID'].apply(np.array).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65847957",
   "metadata": {},
   "source": [
    "#### calculating optimal number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6104bfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating optimal number of clusters... 53%\n"
     ]
    }
   ],
   "source": [
    "if calculate_optimal_clusters_number:\n",
    "    # Find the optimal number of clusters using the elbow method\n",
    "    OPTIMAL_CLUSTERS_NUMBER = find_optimal_num_clusters(posts_embedding_vectors_arr, MIN_CLUSTERS, MAX_CLUSTERS)\n",
    "    optimal_clusters_num_df = pd.DataFrame({\"-\":\"Optimal number of clusters\", 'Value': [str(OPTIMAL_CLUSTERS_NUMBER)]}).set_index('-')\n",
    "    optimal_clusters_num_df.to_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\")\n",
    "else:\n",
    "    print(\"Calculating optional number of clusters proccess is disabled!.\")\n",
    "    # reading optiomal number of clusters from last calculation\n",
    "    OPTIMAL_CLUSTERS_NUMBER = pd.read_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\")[\"Value\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfea8e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing optimal number of clusters using dataframe\n",
    "optimal_clusters_num_df = pd.read_csv(f\"{script_outputs_folder_url}/optimal-clusters-number.csv\").set_index('-')\n",
    "display(optimal_clusters_num_df)\n",
    "\n",
    "# reading optimal clusters number calculating results df\n",
    "optimal_clustering_details_df = pd.read_csv(f\"{script_outputs_folder_url}/optimal-num-of-clusters-calculation-results.csv\")\n",
    "# Plot the elbow curve and clusters covered percentage\n",
    "print_optimal_num_of_clusters_calculation_graphs(optimal_clustering_details_df[\"clusters_number\"],\n",
    "                                                 optimal_clustering_details_df[\"wcss\"],\n",
    "                                                 optimal_clustering_details_df[\"clusters_percentage_covered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab2aba",
   "metadata": {},
   "source": [
    "#### building kmeans model with optimal number of clusters found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aadf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KMeans model with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=OPTIMAL_CLUSTERS_NUMBER, random_state=42)\n",
    "    \n",
    "# Fit KMeans model to the vectors\n",
    "kmeans.fit(posts_embedding_vectors_arr)\n",
    "\n",
    "# Get cluster labels assigned to each vector\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "\n",
    "# extracting posts indexes per cluster\n",
    "# --> posts_per_cluster_lst[i] will include posts indexes that belong to cluster i\n",
    "\n",
    "posts_indexes_per_cluster_lst = list()\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    posts_indexes_per_cluster_lst.append([index for index, value in enumerate(cluster_labels) if value == cluster])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74091a60",
   "metadata": {},
   "source": [
    "#### covered clusters (by snowballed profiles) percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e1af1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating snowballed profiles quality percent\n",
    "percent = calculate_clusters_covered_percent_by_profiles(profiles_id_arr, OPTIMAL_CLUSTERS_NUMBER, cluster_labels)\n",
    "\n",
    "#printing result using dataframe\n",
    "display(pd.DataFrame({\"-\":\"Clusters Covered\", 'Percentage': [\"{:.2f}\".format(percent)+\"%\"]}).set_index('-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a32ad",
   "metadata": {},
   "source": [
    "#### i'th profiles quality grapth (covered clusters percent for the first i'th profiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f4275c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calculate_ith_profiles_clusters_cover:\n",
    "    # reading posts Owners_Id column from posts dataframe into a nparray\n",
    "    posts_owners_id_arr = np.array(posts_df['Owner_ID'].apply(np.array).tolist())\n",
    "\n",
    "    # list to save percentage results\n",
    "    percentage_lst = list()\n",
    "\n",
    "    # calculating profiles dataframe length\n",
    "    profiles_count = len(profiles_id_arr)\n",
    "\n",
    "    for curr_num_of_profiles in range(1, profiles_count + 1):\n",
    "        # initializing a set that include all the first i profiles id\n",
    "        curr_profiles_id_set = set(profiles_id_arr[:curr_num_of_profiles])\n",
    "\n",
    "        #scanning clusters to check how many were covered by the first ith profiles\n",
    "        clusters_covered_count = 0\n",
    "        for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "            # extracting posts owners id in the current cluster\n",
    "            posts_owners_id_in_cluster_set = set([posts_owners_id_arr[idx] for idx in posts_indexes_per_cluster_lst[cluster]])\n",
    "            # checking if the cluster was covered by the current profiles set\n",
    "            intersaction_set = curr_profiles_id_set.intersection(posts_owners_id_in_cluster_set)\n",
    "            if len(intersaction_set) > 0:\n",
    "                clusters_covered_count = clusters_covered_count + 1\n",
    "\n",
    "        percentage_lst.append((clusters_covered_count / OPTIMAL_CLUSTERS_NUMBER) * 100)\n",
    "        #printing process percentage\n",
    "        print_process(\"Calculating profiles quality...\", curr_num_of_profiles, profiles_count)\n",
    "\n",
    "\n",
    "    # creating a list that include the number 1 to len(profiles_df)\n",
    "    amount_of_profiles_lst = list(range(1, len(profiles_id_arr) + 1))\n",
    "\n",
    "    # creating dataframe with results\n",
    "    covered_clusters_results_df = pd.DataFrame({'profiles_count': amount_of_profiles_lst,\n",
    "                                   'percentage': percentage_lst})\n",
    "    #saving results into a csv file\n",
    "    covered_clusters_results_df.to_csv(f\"{script_outputs_folder_url}/covered-clusters-i-profiles.csv\")\n",
    "\n",
    "\n",
    "# plot first i'th profiles cover results graph\n",
    "covered_clusters_results_df = pd.read_csv(f\"{script_outputs_folder_url}/covered-clusters-i-profiles.csv\")\n",
    "plt.plot(covered_clusters_results_df[\"profiles_count\"], covered_clusters_results_df[\"percentage\"])\n",
    "plt.xlabel(\"amount of profiles\")  # Set x-axis label\n",
    "plt.ylabel(\"Clusters Covered percentage %\")  # Set y-axis label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b6761",
   "metadata": {},
   "source": [
    "#### creating clusters captions dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_captions_list = list()\n",
    "all_captions_list = posts_df[CAPTIONS_COLUMN_NAME]\n",
    "columns_names = list()\n",
    "\n",
    "# scanning clusters to save each post caption in specific cluster\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # extracting all captions that belongs to the current cluster\n",
    "    cluster_captions_list = [all_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "    columns_names.append(f\"cluster_{cluster+1}\")\n",
    "    \n",
    "\n",
    "# Transpose the list of lists to switch rows with columns\n",
    "transposed_lists = list(map(list, zip(*cluster_captions_list)))\n",
    "# Create a DataFrame from the transposed list with optional column names\n",
    "clusters_captions_df = pd.DataFrame(transposed_lists, columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a892b",
   "metadata": {},
   "source": [
    "#### saving dataframe into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dca124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_captions_df.info()\n",
    "display(clusters_captions_df.head(5))\n",
    "clusters_captions_df.to_parquet(f\"{script_outputs_folder_url}/clusters-captions.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f59076",
   "metadata": {},
   "source": [
    "## Naming clusters\n",
    "#### --------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d5dacf",
   "metadata": {},
   "source": [
    "### method 1: most common triple words in cluster (using black list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30aa92",
   "metadata": {},
   "source": [
    "#### Preparing a blacklist of words that we wish to exclude from our cluster name tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f74b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\"a\", \"an\", \"the\"]\n",
    "conjunctions = [\"and\", \"but\", \"or\", \"nor\", \"for\", \"yet\", \"so\", \"after\", \"although\", \"as\", \"because\", \"before\", \"if\", \"since\", \"though\", \"unless\", \"until\", \"when\", \"where\", \"while\"]\n",
    "prepositions = [\"aboard\", \"about\", \"above\", \"across\", \"after\", \"against\", \"along\", \"amid\", \"among\", \"around\",\n",
    "                \"as\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"but\",\n",
    "                \"by\", \"concerning\", \"considering\", \"despite\", \"down\", \"during\", \"except\", \"excepting\",\n",
    "                \"excluding\", \"following\", \"for\", \"from\", \"in\", \"inside\", \"into\", \"like\", \"near\", \"of\", \"off\",\n",
    "                \"on\", \"onto\", \"out\", \"outside\", \"over\", \"past\", \"regarding\", \"round\", \"since\", \"through\",\n",
    "                \"throughout\", \"till\", \"to\", \"toward\", \"towards\", \"under\", \"underneath\", \"until\", \"unto\",\n",
    "                \"up\", \"upon\", \"versus\", \"via\", \"with\", \"within\", \"without\"]\n",
    "pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"you\", \"him\", \"her\", \"us\", \"them\",\n",
    "            \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\",\n",
    "            \"who\", \"whom\", \"whose\", \"which\", \"what\", \"that\", \"whichever\", \"whatever\", \"whoever\", \"whomever\",\n",
    "            \"this\", \"these\", \"those\", \"someone\", \"somebody\", \"something\", \"anyone\", \"anybody\", \"anything\",\n",
    "            \"everyone\", \"everybody\", \"everything\", \"no one\", \"nobody\", \"nothing\", \"each\", \"either\", \"neither\",\n",
    "            \"one\", \"other\", \"another\", \"such\", \"much\", \"few\", \"both\", \"all\", \"any\", \"some\", \"several\", \"none\",\n",
    "            \"every\", \"many\", \"more\", \"most\", \"enough\", \"little\", \"fewer\", \"fewest\", \"least\", \"I\", \"you\", \"he\",\n",
    "            \"she\", \"it\", \"we\", \"they\"]\n",
    "possessive_determiners = [\"my\", \"your\", \"his\", \"her\", \"its\", \"our\", \"your\", \"their\"]\n",
    "randoms = [\"is\", \"are\", \"yes\", \"no\", \"http\", \"https\", \"has\", \"had\", \"will\", \"would\", \"was\", \"were\", \"not\",\n",
    "           \"be\", \"just\", \"been\", \"do\", \"does\", \"has\", \"have\"]\n",
    "\n",
    "common_words_set = set(articles + conjunctions + prepositions + pronouns + possessive_determiners + randoms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee111b0",
   "metadata": {},
   "source": [
    "#### coverting each caption into unique words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74131983",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# a list to store aggresive cleaned posts text - removing symbols and numbers\n",
    "clean_captions_list = list()\n",
    "# a list to save each caption words \n",
    "clean_captions_unique_words_list = list()\n",
    "\n",
    "# scanning posts dataframe to aggresive clean posts captions\n",
    "for idx, row in posts_df.iterrows():\n",
    "    # aggresive cleaning the texts, removing all numbers and symbols\n",
    "    # reading caption\n",
    "    caption = row[CAPTIONS_COLUMN_NAME]\n",
    "    # removing all non-alphabeta chars\n",
    "    clean_caption = ''.join([char if char.isalpha() else ' ' for char in caption])\n",
    "    clean_caption = ' '.join([word for word in clean_caption.split() if (word.isalpha() and len(word) >=3)])\n",
    "    clean_captions_list.append(clean_caption)\n",
    "\n",
    "\n",
    "for i in range(len(clean_captions_list)):\n",
    "    # coverting clean post texts into a unique list of words\n",
    "    clean_captions_unique_words_list.append(list(set([word for word in clean_captions_list[i].lower().split() if (word not in common_words_set)])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508cb335",
   "metadata": {},
   "source": [
    "#### scanning clusters to find most common 3 words tupple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51293dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list to save clusters name\n",
    "clusters_name_lst = list()\n",
    "\n",
    "# scanning clusters\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # init a dictionary to count words tuple appearances\n",
    "    words_tupple_count_dict = dict()\n",
    "    # extracting cluster caption sentences (each caption is a list of words)\n",
    "    cluster_captions_words = [clean_captions_unique_words_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "\n",
    "    # scanning all captions in cluster to count words tuples appearances\n",
    "    for caption_words in cluster_captions_words:\n",
    "        # checking if caption has enough words\n",
    "        if len(caption_words) >= 3:\n",
    "            for idx1 in range(len(caption_words)-2):\n",
    "                for idx2 in range(idx1+1, len(caption_words)-1):\n",
    "                    for idx3 in range(idx2+1, len(caption_words)):\n",
    "                        words_tuple = (caption_words[idx1], caption_words[idx2], caption_words[idx3])\n",
    "                        current_count = words_tupple_count_dict.get(words_tuple, 0)\n",
    "                        words_tupple_count_dict[words_tuple] = current_count + 1\n",
    "    \n",
    "    # adding cluster name tuple to list\n",
    "    clusters_name_lst.append(max(words_tupple_count_dict, key=words_tupple_count_dict.get))\n",
    "    \n",
    "    # printing proccess percentage\n",
    "    print_process(\"Producing names to clusters...\", cluster, OPTIMAL_CLUSTERS_NUMBER - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d4cc8",
   "metadata": {},
   "source": [
    "#### printing clusters names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06eca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_names_df = pd.DataFrame({'cluster number':[i for i in range(OPTIMAL_CLUSTERS_NUMBER)],\n",
    "                                  'cluster name': clusters_name_lst }).set_index(\"cluster number\")\n",
    "\n",
    "clusters_names_file_url = data_set_folder_url +\"cluster-names.csv\"\n",
    "clusters_names_df.to_csv(clusters_names_file_url)\n",
    "display(clusters_names_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6a9ab",
   "metadata": {},
   "source": [
    "#### Creating cluster's captions dataframe, saving it to file and printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2de39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e522dc",
   "metadata": {},
   "source": [
    "### method 2: td-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141297e",
   "metadata": {},
   "source": [
    "#### creating clusters decuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf4c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_documents_list = list()\n",
    "all_captions_list = posts_df[CAPTIONS_COLUMN_NAME]\n",
    "\n",
    "# scanning clusters to build document for each cluster\n",
    "for cluster in range(OPTIMAL_CLUSTERS_NUMBER):\n",
    "    # extracting all captions that belongs to the current cluster\n",
    "    cluster_captions_list = [all_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "    # creating current cluster document (appending all captions togther)\n",
    "    clusters_documents_list[cluster] = ' '.join(cluster_captions_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.5)\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "# the return value is a matrix:\n",
    "#   - row i describe the tfidf score of each word in document i\n",
    "#   - column j describe the tfidf score for word j in each document\n",
    "# the order of the columns correspond to the words list we receive from vectorizer.get_feature_names_out() function\n",
    "tfidf_score_array = vectorizer.fit_transform(clusters_documents_list).toarray()\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf score dataframe\n",
    "# - column i include the tf-idf score for term i in all documents\n",
    "# - row i include terms score in document i\n",
    "# terms_tfidf_score_in_documents_df(i,j) = tf-idf score for term j in document i\n",
    "terms_tfidf_score_in_documents_df = pd.DataFrame(tfidf_score_array, columns = feature_names)\n",
    "\n",
    "display(terms_tfidf_score_in_documents_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc19a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,word in enumerate(feature_names):\n",
    "    print(str(i)+\". \"+word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d895f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==0, \"end of program :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b5f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45800ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some code that would be needed for td-idf\n",
    "\n",
    "# dict to each cluster captions (key-cluster number, value-captions list)\n",
    "# will be used to save clusters caption in csv file and for tf-idf documents\n",
    "cluster_captions_dict = dict()\n",
    "\n",
    "# saving clusters clean captions\n",
    "cluster_captions = [clean_captions_list[idx] for idx in posts_indexes_per_cluster_lst[cluster]]\n",
    "cluster_captions_dict[cluster_dict_key] = cluster_captions\n",
    "\n",
    "# soft clean code\n",
    "\n",
    "clean_caption = ''.join([char for char in row[CAPTIONS_COLUMN_NAME] if ord(char) < 128])\n",
    "# removing all extra spaces and new-lines\n",
    "clean_caption = ' '.join([word for word in clean_caption.split()])\n",
    "\n",
    "\n",
    "### need to remmber what does this code do!\n",
    "# a list to store terms-score dictionary for each cluster\n",
    "cluster_terms_score_dict_list = list()\n",
    "\n",
    "for idx in range(len(tfidf_score_array)):\n",
    "    cluster_terms_score_dict = dict(zip(feature_names, tfidf_score_array[idx]))\n",
    "    # removing common words from dict\n",
    "    cluster_terms_score_dict = {k: v for k, v in cluster_terms_score_dict.items() if k not in common_words_set}\n",
    "    # Sort the dictionary by values in descending order\n",
    "    cluster_terms_score_dict = sorted(cluster_terms_score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    # append dictionary to dictionaries list\n",
    "    cluster_terms_score_dict_list.append(cluster_terms_score_dict)\n",
    "    \n",
    "cluster_terms_score_dict_list[1]\n",
    "\n",
    "#### code to create df which include clusters captions\n",
    "# creating df from dict\n",
    "cluster_captions_df = pd.concat({key: pd.Series(vals) for key, vals in cluster_captions_dict.items()}, axis=1)\n",
    "# saving into csv file\n",
    "clusters_caption_file_url = data_set_folder_url +\"clusters-captions.csv\"\n",
    "cluster_captions_df.to_csv(clusters_caption_file_url)\n",
    "# display dataframe for fast preview\n",
    "display(cluster_captions_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca22b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf248f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f65f15b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = ['data science is one of the most important fields of science',\n",
    "          'this is one of the best data science courses',\n",
    "          'data scientists analyze data' ]\n",
    "\n",
    "tr_idf_model  = TfidfVectorizer(max_df = 0.5)\n",
    "tf_idf_array = tr_idf_model.fit_transform(corpus).toarray()\n",
    "words_set = tr_idf_model.get_feature_names_out()\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = words_set)\n",
    "\n",
    "display(df_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e2378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "words_set = set()\n",
    "\n",
    "for doc in  documents:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "n_docs = len(documents)         #·Number of documents in the corpus\n",
    "n_words_set = len(words_set) #·Number of unique words in the \n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), columns=words_set)\n",
    "\n",
    "# Compute Term Frequency (TF)\n",
    "for i in range(n_docs):\n",
    "    words = corpus[i].split(' ') # Words in the document\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576fc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (muhamenv)",
   "language": "python",
   "name": "muhamenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
